---
title: "H2O Tutorial"
author: "Dol_R"
date: "8/30/2020"
output: 
  html_document:
    toc: true
    toc_depth: 3
---

## Introduction
`h2o` is an open source Machine Learning library that can be used in both R and Python. It provides easy and fast solutions for large datasets. It is integrated with R, Python etc. In this tutorial we will demonstrate how to use this package for machine learning.  

Normally we follow these steps:
- Data
- Train/Test Split
- Apply Model on Training Set
- Prediction on Test Set
- Accuracy

With H2O, we also need to initialize the package and convert data into H2O frame. 

## Setup: Install and Load the Library
First, copy the code in [h2o website](http://h2o-release.s3.amazonaws.com/h2o/rel-shannon/26/index.html#R) and paste into the console. 

```{r setup, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
#install one time only
#install.packages("h2o")
#install bit64, otherwise it gives an error
#install.packages('bit64', repos = "https://cran.rstudio.com")
library(bit64)
library(tidyverse)
library(ggplot2)
library(h2o)
## Initialize h2o cluster
h2o.init()
```

## Iris Data
Iris is a built-in dataset in R. It has 4 numerical independent variables and a categorical independent variable.
This data has `r dim(iris)[1]` rows and `r dim(iris)[2]` columns. Dependent variable is `Species`.

```{r data}
data <- iris
glimpse(data)
data %>% count(Species)
```

```{r frame, results='hide'}
# Convert data to h2o frame
iris_data <- as.h2o(iris)
# Identify target and features
y <- "Species"
x <- setdiff(colnames(iris_data), y)
# For classification target should be a factor
iris_data[, y] <- as.factor(iris_data[, y])
```

```{r descb}
h2o.describe(iris_data)
```


### Train Test Split
```{r split}
# 80% train, 20% test
# Seed is defined in order to get the same result
split = h2o.splitFrame(data = iris_data, ratios = 0.8, seed=90)
train = split[[1]]
test = split[[2]]
```

### Model: gbm
```{r model, results='hide'}
# x=features, y=target
gbm_model <- h2o.gbm(x=x, y=y, training_frame = train, seed=90)
```

```{r perf,  warning=FALSE}
# Model's performance using test data, includes R^2 and confusion matrix
perf <- h2o.performance(gbm_model,test)
perf
```

### Model2: Random Forest
```{r model2, results='hide'}
# x=features, y=target
random_forest_model <- h2o.randomForest(x=c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width"), y="Species", training_frame = train, seed=90)
```

```{r perf2}
# Model's performance using test data, includes R^2 and confusion matrix
perf2 <- h2o.performance(random_forest_model,test)
perf2
```

We got very similar results. 

## GBM Hyperparameters

```{r }
```

## Resources
- [Cran](https://cran.r-project.org/web/packages/h2o/index.html)
- [h2o.ai](http://docs.h2o.ai/h2o-tutorials/latest-stable/WhatIsH2O.html)
- [H2O Documentation](http://docs.h2o.ai/)
- [Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud)
- [Generalized Linear Model(GLM)](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html#)
- [Datacamp](https://campus.datacamp.com/courses/hyperparameter-tuning-in-r/hyperparameter-tuning-with-h2o?ex=1)
