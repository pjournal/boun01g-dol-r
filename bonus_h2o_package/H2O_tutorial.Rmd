---
title: "H2O Tutorial"
author: "Dol_R"
date: "8/30/2020"
output: 
  html_document:
    toc: true
    toc_depth: 3
---

## Introduction

`H2O` is a fully open source, distributed in-memory machine learning platform with linear scalability. H2O supports the most widely used statistical & machine learning algorithms including gradient boosted machines, generalized linear models, deep learning and more. H2O also has an industry leading AutoML functionality that automatically runs through all the algorithms and their hyperparameters to produce a leaderboard of the best models. The H2O platform is used by over 18,000 organizations globally and is extremely popular in both the R & Python communities.

## Leading Algorithms

Algorithms developed from the ground up for distributed computing and for both supervised and unsupervised approaches including Random Forest, GLM, GBM, XGBoost, GLRM, Word2Vec and many more.

## H2O Open Source AutoML

Train the best model in the least amount of time to save human hours.
Reduce the need for expertise in machine learning by reducing the manual code-writing time.
Improve the performance of machine learning models.
Increase reproducibility and establish a baseline for scientific research or applications.
Scales training data set to clusters (Hadoop, Spark, Kubernetes)



Normally we follow these steps:

* Data
* Train/Test Split
* Apply Model on Training Set
* Prediction on Test Set
* Accuracy

With H2O, we have additional steps to initialize the package and convert data into H2O frame. 

## Setup: Install and Load the Library
First, copy the code in [h2o website](http://h2o-release.s3.amazonaws.com/h2o/rel-shannon/26/index.html#R) and paste it into the console. 

```{r setup, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
#install one time only
#install.packages("h2o")
#install bit64, otherwise it gives an error
#install.packages('bit64', repos = "https://cran.rstudio.com")
library(bit64)
library(tidyverse)
library(ggplot2)
library(h2o)
# Initialize h2o cluster
h2o.init()
# To connect to an established H2O cluster
# h2o.init(ip = "123.45.67.89", port = 54321)

# Cluster info
h2o.clusterInfo()
```

## Data Preparation
Data preparation/manipulation can be made using the methods available in h2o package.

* `as.data.frame()`: Converts H2O data frame into R data frame
* `as.h2o()`: Transfers data from R to the H2O instance
* `str.H2OFrame()`: Returns the elements of the new object
* `h2o.describe()` : Returns information about column types

## Models

Some of the models H2O supports:

* Generalized Linear Models (GLM)
* Gradient Boosting Machine (GBM)
* Distributed Random Forest (DRF)
* Principal Components Analysis (PCA)
* Deep Learning
* K-means

## Example: Iris Data
Iris is a built-in dataset in R. It has 4 numerical independent variables and a categorical independent variable. Therefore this is a classification problem, not a regression.
This data has `r dim(iris)[1]` rows and `r dim(iris)[2]` columns.

```{r data}
data <- iris
glimpse(data)
data %>% count(Species)
```

```{r frame, results='hide'}
# Convert data to h2o frame
iris_data <- as.h2o(iris)
```

```{r colnames}
# Column names of data
colnames(iris_data)
```


```{r descb}
h2o.describe(iris_data)
```

### Train Test Split
H2O package has ` h2o.splitFrame()` function to split the H2O data frame.

```{r split}
# 80% train, 20% test
# Seed is defined in order to get the same result
split = h2o.splitFrame(data = iris_data, ratios = 0.8, seed=90)
train = split[[1]]
test = split[[2]]
```

### Model1: Gradient Boosting Machine(GBM)
Here the dependent variable is `Sepal.Length` and it's numerical.
After fitting the model, we can reach some statistics about it. One of the most important statistic is Mean Squared Error (MSE). 
```{r model1, results='hide'}
# x=features, y=target, we chose the target to be Sepal.Length
gbm_model <- h2o.gbm(x=2:5, y=1, training_frame = train,  ntrees = 10, max_depth = 3,min_rows = 2, learn_rate = 0.2, distribution= "gaussian", seed=90)
```

```{r model1.}
gbm_model
```

We could also predict the categorical variable `Species`. Here we can see the confusion matrix.

```{r model1.2, results='hide'}
# x=features, y=target, we chose the target to be Species
gbm_model2 <- h2o.gbm(y = 5, x = 1:4, training_frame
= train, ntrees = 15, max_depth = 5, min_rows =
2, learn_rate = 0.01, distribution= "multinomial", seed=90)
```

```{r model1.2.}
gbm_model2
```

```{r perf,  warning=FALSE}
# Model's performance using test data, includes R^2 and confusion matrix
perf <- h2o.performance(gbm_model,test)
perf
perf2 <- h2o.performance(gbm_model2,test)
perf2
```

### Model2: Generalized Linear Models (GLM)

Let's fit Random Forest model into our `train` set and try to predict `Species`.

```{r model2, results='hide'}
# x=features, y=target
glm_model <- h2o.glm(x=c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width"), y="Species", training_frame = train, family ="multinomial",  nfolds = 10, seed=90)
```

```{r model2.}
glm_model
```

```{r perf2, warning=FALSE}
# Model's performance using test data, includes R^2 and confusion matrix
perf3 <- h2o.performance(glm_model,test)
perf3
```

### Comparison of the Models
To compare the classification models we look at the confusion matrices of the test data.
Both of the methods work very well with 1 or 2 errors.



## Resources
- [Cran](https://cran.r-project.org/web/packages/h2o/index.html)
- [h2o.ai](http://docs.h2o.ai/h2o-tutorials/latest-stable/WhatIsH2O.html)
- [H2O Documentation](http://docs.h2o.ai/)
- [Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud)
- [Generalized Linear Model(GLM)](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html#)
- [Datacamp](https://campus.datacamp.com/courses/hyperparameter-tuning-in-r/hyperparameter-tuning-with-h2o?ex=1)
- [h2o.ai RBooklet](https://www.h2o.ai/wp-content/uploads/2018/01/RBooklet.pdf)